{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../../')\n",
    "import wandb\n",
    "import pathlib\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import tempfile\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_CACHE = str(pathlib.PosixPath('~/.wandb_cache').expanduser())\n",
    "VERSION     =\"v2\" ##<- update if you want to use a different verison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = os.path.join(WANDB_CACHE,\"aaac_raw:%s\" % VERSION)\n",
    "def grab_raw_data(path):\n",
    "    if not os.path.isdir(FILE_PATH):\n",
    "        api = wandb.Api()\n",
    "        artifact = api.artifact(\n",
    "            'aaac/dataset_versions/aaac_raw:%s' % VERSION, \n",
    "            type='raw_data'\n",
    "        )\n",
    "        artifact_dir = artifact.download(root=FILE_PATH)\n",
    "        #with wandb.init(entity=\"aaac\",project=\"dataset_versions\",name=\"dataset_download\") as run: \n",
    "            #artifact = run.use_artifact(\n",
    "            #    'aaac/dataset_versions/aaac_raw:%s' % VERSION, \n",
    "            #    type='raw_data'\n",
    "            #)\n",
    "grab_raw_data(FILE_PATH) \n",
    "DATA_JSON=os.path.join(FILE_PATH,\"aaac.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grabs the raw `aaac` corpus (with version `v0`) from wandb and places it into a wandb cache. It first requires having some global access to your `WANDB_API_KEY`, which can be set by doing `export WANDB_API_KEY=....` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINES = []\n",
    "with open(DATA_JSON) as my_data: \n",
    "    for line in my_data: \n",
    "        line      = line.strip()\n",
    "        json_line = json.loads(line)\n",
    "        LINES.append(json_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "len(LINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'text': 'Lisa occasionally purchases Camay soap or Lisa regularly consumes Bentley Organic soap',\n",
       "  'starts_at': 160,\n",
       "  'ref_reco': 1},\n",
       " {'text': 'if Lisa occasionally purchases Camay soap, then Lisa occasionally purchases Infusium shampoo',\n",
       "  'starts_at': 252,\n",
       "  'ref_reco': 2},\n",
       " {'text': 'if Lisa regularly consumes Bentley Organic soap, then Lisa occasionally purchases Infusium shampoo',\n",
       "  'starts_at': 494,\n",
       "  'ref_reco': 3},\n",
       " {'text': 'Lisa occasionally purchases Protex soap',\n",
       "  'starts_at': 607,\n",
       "  'ref_reco': 5}]"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "LINES[1][\"reason_statements\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['argument_source',\n",
       " 'argdown_reconstruction',\n",
       " 'reason_statements',\n",
       " 'conclusion_statements',\n",
       " 'premises',\n",
       " 'premises_formalized',\n",
       " 'conclusion',\n",
       " 'conclusion_formalized',\n",
       " 'intermediary_conclusions_formalized',\n",
       " 'intermediary_conclusions',\n",
       " 'distractors',\n",
       " 'id',\n",
       " 'predicate_placeholders',\n",
       " 'entity_placeholders',\n",
       " 'steps',\n",
       " 'n_premises',\n",
       " 'n_distractors',\n",
       " 'base_scheme_groups',\n",
       " 'scheme_variants',\n",
       " 'domain_id',\n",
       " 'domain_type',\n",
       " 'plcd_subs',\n",
       " 'argdown_index_map',\n",
       " 'presentation_parameters']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "list(LINES[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines how to present reason and conclusion statements to the model\n",
    "def format_statements_list(statements: list) -> str:\n",
    "    if len(statements)==0:\n",
    "        return \"None\"\n",
    "    list_as_string = [\"%s (ref: (%s))\" % (sdict['text'],sdict['ref_reco']) for sdict in statements]\n",
    "    list_as_string = \" | \".join(list_as_string)\n",
    "    return list_as_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Lisa occasionally purchases Infusium shampoo and Lisa occasionally purchases Protex soap (ref: (6)) | Lisa occasionally purchases Infusium shampoo (ref: (4))'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "format_statements_list(LINES[1][\"conclusion_statements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines how to present argdown-snippe to the model\n",
    "def format_argdown(argdown: str) -> str:\n",
    "    argdown = argdown\n",
    "    pattern = r\"({.*uses: \\[[\\s\\d,]*\\]})\" # matches yaml metadata inline blocks in inference patterns \n",
    "    matches = re.findall(pattern, argdown)\n",
    "    for match in matches:\n",
    "        m = match.replace('uses:','\"uses\":')\n",
    "        m = m.replace('variant:','\"variant\":')\n",
    "        d = ast.literal_eval(m)\n",
    "        subst = \"\" \n",
    "        if \"variant\" in d:\n",
    "            subst = \"(%s) \" % \", \".join(d['variant'])\n",
    "        subst = subst + \"from \" + \" \".join([\"(%d)\" % i for i in d['uses']])\n",
    "        argdown = argdown.replace(match,subst)\n",
    "    return argdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1) If something is not a street longer than Jefferson Street, then it is not a street crossing State Street.\n--\nwith instantiation (transposition) from (1)\n--\n(2) If River Street is a street crossing State Street, then River Street is a street longer than Jefferson Street.\n(3) If River Street is a street longer than Jefferson Street, then River Street is a street crossing Summit Avenue.\n--\nwith chain rule from (2) (3)\n--\n(4) If River Street is a street crossing State Street, then River Street is a street crossing Summit Avenue.\n(5) River Street is a street crossing State Street or River Street is a street longer than Front Street.\n(6) If River Street is a street longer than Front Street, then River Street is a street crossing Summit Avenue.\n--\nwith case analysis from (4) (5) (6)\n--\n(7) River Street is a street crossing Summit Avenue.\n"
     ]
    }
   ],
   "source": [
    "print(format_argdown(LINES[0][\"argdown_reconstruction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# define modes in terms of keys\n",
    "modes = [\n",
    "    {'from':['argument_source'],'to':'argdown_reconstruction'},\n",
    "    {'from':['argument_source','reason_statements'],'to':'argdown_reconstruction'},\n",
    "    {'from':['argument_source','conclusion_statements'],'to':'argdown_reconstruction'},\n",
    "    {'from':['reason_statements','conclusion_statements'],'to':'argdown_reconstruction'},\n",
    "    {'from':['argument_source','reason_statements','conclusion_statements'],'to':'argdown_reconstruction'},\n",
    "    {'from':['argument_source'],'to':'reason_statements'},\n",
    "    {'from':['argument_source','argdown_reconstruction'],'to':'reason_statements'},\n",
    "    {'from':['argument_source','conclusion_statements'],'to':'reason_statements'},\n",
    "    {'from':['argument_source'],'to':'conclusion_statements'},\n",
    "    {'from':['argument_source','argdown_reconstruction'],'to':'conclusion_statements'},\n",
    "    {'from':['argument_source','reason_statements'],'to':'conclusion_statements'},\n",
    "]\n",
    "len(modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_amount = int(len(LINES)*0.7)\n",
    "eval_amount  = int(len(LINES)*0.15)\n",
    "random.shuffle(LINES)\n",
    "train_instances = LINES[:train_amount]\n",
    "dev_instances   = LINES[train_amount:train_amount+eval_amount]\n",
    "test_instances  = LINES[train_amount+eval_amount:]\n",
    "max_words = 280"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just took a random train/test/dev split to start with. Not sure how much this makes sense given the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maaac\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.29 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.12<br/>\n                Syncing run <strong style=\"color:#cdcd00\">dataset_upload</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/aaac/dataset_versions\" target=\"_blank\">https://wandb.ai/aaac/dataset_versions</a><br/>\n                Run page: <a href=\"https://wandb.ai/aaac/dataset_versions/runs/26iw4gqs\" target=\"_blank\">https://wandb.ai/aaac/dataset_versions/runs/26iw4gqs</a><br/>\n                Run data is saved locally in <code>/Users/ggbetz/git/transformer_tools/notebooks/wandb/run-20210507_084727-26iw4gqs</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/var/folders/ql/h_s52yl51x70ynttgg820gz80000gp/T/tmp4uceqtpc)... Done. 0.2s\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 2185<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 124.81MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=8.458579657…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a44551a587374fa6bdbae70ff76ed005"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/Users/ggbetz/git/transformer_tools/notebooks/wandb/run-20210507_084727-26iw4gqs/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/Users/ggbetz/git/transformer_tools/notebooks/wandb/run-20210507_084727-26iw4gqs/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">dataset_upload</strong>: <a href=\"https://wandb.ai/aaac/dataset_versions/runs/26iw4gqs\" target=\"_blank\">https://wandb.ai/aaac/dataset_versions/runs/26iw4gqs</a><br/>\n                "
     },
     "metadata": {}
    }
   ],
   "source": [
    "##open wandb again\n",
    "run = wandb.init(entity=\"aaac\",project=\"dataset_versions\",name=\"dataset_upload\")\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tempdir: \n",
    "    for sname,split in [\n",
    "        (\"train\",train_instances),\n",
    "        (\"dev\",dev_instances),\n",
    "        (\"test\",test_instances)\n",
    "    ]:\n",
    "        ### outputfile \n",
    "        over = 0\n",
    "        total = 0\n",
    "        \n",
    "        file_out = os.path.join(tempdir,sname+\".jsonl\")\n",
    "        write_file = open(file_out,'w')\n",
    "    \n",
    "        for k,instance in enumerate(split):\n",
    " \n",
    "        \n",
    "            ### iterate over all modes\n",
    "            for mode in modes:\n",
    "                mname = \"+\".join(mode['from']) +'>'+mode['to']\n",
    "\n",
    "                # construct input\n",
    "                question=\"\"\n",
    "                for key_from in mode['from']:\n",
    "                    add = instance[key_from]\n",
    "                    if key_from in [\"reason_statements\",\"conclusion_statements\"]:\n",
    "                        add = format_statements_list(add)\n",
    "                    elif key_from in [\"argdown_reconstruction\"]:\n",
    "                        add = format_argdown(add)\n",
    "                    question = question + \" %s: %s\" % (key_from,add)\n",
    "                #question = question + \" \" + mode['to'] +\":\" # mode['to'] is used as prefix\n",
    "                question = question.strip()\n",
    "\n",
    "\n",
    "                # construct output\n",
    "                output=instance[mode['to']]\n",
    "                if mode['to'] in [\"reason_statements\",\"conclusion_statements\"]:\n",
    "                    output = format_statements_list(output)\n",
    "                elif mode['to'] in [\"argdown_reconstruction\"]:\n",
    "                    output = format_argdown(output)\n",
    "                output = output.strip()\n",
    "\n",
    "                ### arbitrary limitation on input size for now\n",
    "                ## transformer is limited here\n",
    "                if len(question.split()) <= max_words and len(output.split()) <= max_words: \n",
    "                    # put input and output together\n",
    "                    ### json line format and schema for Kyle's model \n",
    "                    new_item = {}\n",
    "                    new_item[\"id\"] = \"%s_%d_%s\" % (sname,k,mname)\n",
    "                    new_item[\"question\"] = {}\n",
    "                    new_item[\"question\"][\"stem\"] = question #<-- input field\n",
    "                    new_item[\"output\"] = output ##<-- left in newlines, tokenizer will ignore them\n",
    "                    new_item[\"prefix\"] = mode['to'] +\":\" ##<-- model specific field, indicates the model mode | \"answer:\" -> using the HACK by Dennis\n",
    "                    write_file.write(json.dumps(new_item))\n",
    "                    write_file.write(\"\\n\")\n",
    "                \n",
    "        write_file.close()\n",
    "        \n",
    "    ### write to wandb \n",
    "    artifact = wandb.Artifact(\"aaac_multi_angle\",type='dataset',metadata={\n",
    "        \"source\": ('aaac/aaac_model_runs/aaac_raw:%s' % VERSION),\n",
    "        \"max_length\": max_words,\n",
    "        \"modes\": str(modes)\n",
    "    })\n",
    "    artifact.add_dir(tempdir)\n",
    "    run.log_artifact(artifact)\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python367jvsc74a57bd0318670b9451e2b0a97a0b451ec166eede77923c420a4d40971c604c0283690ce",
   "display_name": "Python 3.6.7 64-bit ('transformer_tools': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}