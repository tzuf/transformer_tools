description: Train T5-large on babi dataset
tasks:
- spec:
    image: im_5im001j4lcat
    resultPath: /output
    args: [python, -m, transformer_tools, T5Classifier, --output_dir, /output, --data_dir, /inputs, --num_train_epochs, "12", --model_name_or_path, t5-large, --tokenizer_name_or_path, t5-large, --learning_rate, "0.0005", --train_batch_size, "16", --seed, "42", --cloud, --max_seq_len, "250", --max_answer, "10", --early_stopping, --dev_eval, --wdir, /output, --patience, "4", --num_beams, "2", --print_output, --no_repeat_ngram_size, "0",--T5_type, T5ClassificationMultiQA,--data_builder, multi_qa]
    env:
      DATASET : "mixed_babi"
      LR : "0.0005"
      MAX_ITERATIONS : 12
      MODEL : "T5_large"
      BATCH_SIZE : 16
    datasetMounts:
    - datasetId: ds_j878t561da1h
      containerPath: /inputs
    requirements:
      gpuCount: 1
  cluster: us_wvnghctl47k0/01E5TXXY2BN0XNQHF7QHSDJ5YR